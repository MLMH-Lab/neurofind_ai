#!/usr/bin/env python3
"""Script to clean data based on predefined criteria.

This script removes the following data:
    - Subjects with raw data of low quality (where quality metric was calculated by MRIQC [1])
    - Subjects not in the predefined age range (20 <= age <= 80)
    - Datasets only containing patients

References:
    [1] https://mriqc.readthedocs.io/en/stable/
"""
from pathlib import Path

import click
import mlflow
import pandas as pd
import tempfile

from src.definitions import DISEASE_LABEL_LIST
from src.utils import LogRun, PythonLiteralOption


@click.command(help="")
@click.option("--fetch_path")
@click.option("--mriqc_th", default=0.7)
@click.option("--min_age", default=20)
@click.option("--max_age", default=80)
@click.option("--dataset_same_scanner", default=False)
@click.option("--exclude_datasets", cls=PythonLiteralOption, default="[]")
@click.option("--use_only_datasets", cls=PythonLiteralOption, default="[]")
@LogRun()
def clean_data(
    fetch_path,
    mriqc_th=0.7,
    min_age=20,
    max_age=80,
    dataset_same_scanner=False,
    exclude_datasets=[],
    use_only_datasets=[],
):
    """Clean subject list according to criteria.

    Parameters
    ----------
    fetch_path: str
        Path to the fetched and combined data generated by the src.fetch_data script.
    mriqc_th: float, default=0.7
        Minimum quality from MRIQC analysis of the raw data.
    min_age: int, default=20
        Minimum age for subjects to be included.
    max_age: int, default=80
        Maximum age for subjects to be included.
    dataset_same_scanner: bool, default=False

    Outputs
    -------
    participants.tsv: File
        Table separated file containing the subjects' demographic information.
    freesurferData.csv: File
        Comma separated file containing the volumes of the brain regions for each participant.
    qc.tsv: File
        Table separated file containing the quality measures for each of the subjects' image.

    Metrics
    -------
    N_participants: int
        Number of participants.
    N_scanners: int
        Number of different scanners.
    """
    with mlflow.start_run(run_name="clean_data", nested=True) as run:
        print("*********************** CLEAN DATA ***************************")
        # Load original participants file
        participants = pd.read_csv(Path(fetch_path) / "participants.tsv", sep="\t")

        # ----------------------- REMOVE IRRELEVANT DATA ------------------------------------
        # Remove some datasets too granular and/or old
        all_scanners = participants.Dataset.unique()
        if len(exclude_datasets):
            exclude_datasets = [dset for dset in all_scanners if any(
                d in dset for d in exclude_datasets)]
            dset_keep = ~participants.Dataset.isin(exclude_datasets)
        if len(use_only_datasets):
            keep_dset = [dset for dset in all_scanners if any(d in dset for d in use_only_datasets)]
            dset_keep = participants.Dataset.isin(keep_dset)
        if len(exclude_datasets) | len(use_only_datasets):
            participants = participants[dset_keep]

        # Remove subjects below the minimum age and above the maximum age (i.e. keep ages 20-80)
        participants = participants.drop(participants[participants["Age"] < min_age].index)
        participants = participants.drop(participants[participants["Age"] > max_age].index)

        # Remove diagnosis not on DISEASE_LABEL_LIST from participants, but keep the respective controls (label=1)
        participants = participants[(participants["Diagn"].isin(DISEASE_LABEL_LIST + [1]))]

        # Remove multiple sessions and runs
        participants["internal_id"] = participants["Dataset"] + "_" + participants["participant_id"]
        participants = participants.drop_duplicates(subset="internal_id")

        # Sample sizes to save before removing missing data
        n_total_before_removing_na = participants.shape[0]
        n_perscanner_before_removing_na = participants.groupby('Dataset').Diagn.value_counts().to_frame()

        # ----------------------- REMOVE MISSING DATA ------------------------------------
        # Drop subjects with missing data
        participants = participants.dropna()

        # Ensure right format for Diagn and Age columns
        participants.Diagn = participants.Diagn.astype(int)
        participants.Age = participants.Age.astype(int)

        # Sample sizes to save before quality control
        n_total_after_removing_na = participants.shape[0]
        n_perscanner_after_removing_na = participants.groupby('Dataset').Diagn.value_counts().to_frame()

        # ----------------------- REMOVE BAD QUALITY DATA ------------------------------------
        # Quality control using MRIQC
        qc = pd.read_csv(Path(fetch_path) / "qc.tsv", sep="\t",
                         usecols=["id", "mriqc_prob"])
        qc_keep = (qc["mriqc_prob"] < mriqc_th)

        participants = pd.merge(qc[qc_keep], participants, on="id")

        # Sample sizes to save after quality control
        n_total_after_qc = participants.shape[0]
        n_perscanner_after_qc = participants.groupby('Dataset').Diagn.value_counts().to_frame()

        if dataset_same_scanner:
            for dset in [dset.split("-")[0] for dset in participants.Dataset.unique()]:
                participants.loc[participants["Dataset"].str.contains(dset), "Dataset"] = dset

        participants = participants[["id", "participant_id", "Age", "Gender", "Diagn", "Dataset"]]

        # Load FreeSurfer data and MRIQC data
        fs = pd.read_csv(Path(fetch_path) / "freesurferData.csv")
        qc = pd.read_csv(Path(fetch_path) / "qc.tsv", sep="\t")

        fs = pd.merge(fs, participants[["id"]], on="id")
        fs_participants = pd.merge(fs, participants[["id", "Diagn", "Dataset"]], on="id")
        qc = pd.merge(qc, fs[["id"]], on="id")

        # Sample sizes to save after quality control
        n_total_fully_cleaned = fs_participants.shape[0]
        n_perscanner_fully_cleaned = fs_participants.groupby('Dataset', observed=True).Diagn.value_counts().to_frame()

        # Saving MLflow artifacts
        TempDir = tempfile.TemporaryDirectory(dir=run.info.artifact_uri.replace('file://', ''))
        temp_dir = Path(TempDir.name)

        # Save data files
        participants_path = temp_dir / "participants.tsv"
        fs_path = temp_dir / "freesurferData.csv"
        qc_path = temp_dir / "qc.tsv"
        participants.to_csv(participants_path, sep="\t", index=False)
        fs.to_csv(fs_path, index=False)
        qc.to_csv(qc_path, sep="\t", index=False)

        print("Uploading participants.tsv")
        print("Uploading freesurferData.csv")
        print("Uploading qc.tsv")

        mlflow.log_artifact(participants_path)
        mlflow.log_artifact(fs_path)
        mlflow.log_artifact(qc_path)

        # Save sample sizes
        n_perscanner_before_removing_na_path = temp_dir / 'n_perscanner_before_removing_na.csv'
        n_perscanner_after_removing_na_path = temp_dir / 'n_perscanner_after_removing_na.csv'
        n_perscanner_after_qc_path = temp_dir / 'n_perscanner_after_qc.csv'
        n_perscanner_fully_cleaned_path = temp_dir / 'n_perscanner_fully_cleaned.csv'

        n_perscanner_before_removing_na.to_csv(n_perscanner_before_removing_na_path)
        n_perscanner_after_removing_na.to_csv(n_perscanner_after_removing_na_path)
        n_perscanner_after_qc.to_csv(n_perscanner_after_qc_path)
        n_perscanner_fully_cleaned.to_csv(n_perscanner_fully_cleaned_path)

        mlflow.log_artifact(n_perscanner_before_removing_na_path)
        mlflow.log_artifact(n_perscanner_after_removing_na_path)
        mlflow.log_artifact(n_perscanner_after_qc_path)
        mlflow.log_artifact(n_perscanner_fully_cleaned_path)

        mlflow.log_artifacts(temp_dir)

        mlflow.log_metrics(
            # {"N_participants": participants.shape[0], "N_scanners": len(participants.Dataset.unique())})
            {"n_total_before_removing_na": n_total_before_removing_na,
             "n_total_after_removing_na": n_total_after_removing_na,
             "n_total_after_qc": n_total_after_qc,
             "n_total_fully_cleaned": n_total_fully_cleaned
             })

        TempDir.cleanup()
        print("**************************************************************")


if __name__ == "__main__":
    clean_data()
